---
description: Anytime the Agent create / modify any prompt that'll be used by the LLM in our app.
globs: 
alwaysApply: false
---
Ensure every prompt generated by the agent…

1. **Selects the Right Prompting Strategy**  
   - **Intent Classification / Routing:** Use classification prompts with few-shot examples when deciding between high-level intents (email vs calendar vs contact vs general).  
   - **Structured Extraction:** For data-extraction tasks (draft detection, date/time parsing), default to few-shot or multi-shot with strict JSON schemas.  
   - **Instruction-Following:** For transformation tasks (Gmail query builder, summarization), use clear zero-shot instructions with role/context and example I/O where needed.  
   - **Chain-of-Thought & Planning:** For multi-step workflows or complex analysis (RAG, multi-source aggregation), embed “Let’s think step by step” or generate an outline first, then flesh out each step.

2. **Gather Missing Context Proactively**  
   - Before finalizing a prompt, scan required parameters (e.g. draft fields, date ranges, contact info).  
   - If any inputs are missing or ambiguous, insert follow-up questions within the prompt:  
     > ⚠️ **Clarification Request:** “You haven’t provided the event end time. What should it be?”  
   - Structure follow-ups as part of the conversation so they flow naturally.

3. **Include Relevant Context & Role**  
   - Prepend each prompt with a concise system instruction:  
     > *“You are an expert assistant for [task]. Use the tools: [list]. Follow the JSON schema strictly.”*  
   - Append only the necessary slice of conversation history or RAG context to keep the prompt under length limits.

4. **Enforce Output Format & Validation**  
   - Specify the exact output schema (JSON object, keys and types) in the prompt.  
   - Include one or two examples of correctly formatted responses.  
   - Remind the LLM to validate against the schema and to refuse or ask for clarification on invalid inputs.

5. **Leverage Available Tools Effectively**  
   - When a prompt requires external data (emails, calendar events, vector store), indicate in the prompt which tool to call and with what parameters.  
     > *“Retrieve emails from ComposioService.search_emails(query).”*  
   - For RAG tasks, instruct the model to first fetch relevant embeddings from Pinecone, then reason over retrieved documents.

6. **Educate the User & Request Additional Details**  
   - If a prompt requires domain knowledge or preferences (e.g. timezones, templates, tone), end with a **User Prompting Section**:  
     > “Please specify your preferred email tone (formal, casual) and timezone.”  
   - Frame any such questions clearly and concisely.

7. **Maintain Clarity & Positive Framing**  
   - Phrase instructions as positive directives (e.g. “Extract these fields…” rather than “Do not extract anything else”).  
   - Use structured formatting—bullets, numbered steps, or tags—to separate roles, context, and tasks.

8. **Iterate & Refine**  
   - After generating a prompt, simulate an immediate dry-run: ask the model to summarize what it will do next.  
   - If its summary deviates from intent, adjust the prompt (add examples, tighten instructions) before executing.

9. **Log & Tag for Traceability**  
   - Prepend each prompt with metadata tags (e.g. `#PROMPT: draft_detection_v2`, `#MODEL: claude-4-sonnet`).  
   - Include a timestamp and version so you can track and evolve prompts over time.
 
10. **Sync prompt/tooling changes to docs**  
   - When prompts, tool contracts, or model providers change, update the Notion “AI & Automation Strategy” and “API & Integration Documentation” pages (inputs/outputs, guardrails, eval notes).